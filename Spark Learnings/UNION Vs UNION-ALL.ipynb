{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "714d30be-eea9-4b96-b660-d25b839e8ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15d569a-5c05-486b-89da-8bc5914a89c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "\n",
    "mySchema = StructType([StructField(\"id\",IntegerType(),True),\n",
    "                       StructField(\"name\",StringType(),True),\n",
    "                       StructField(\"sal\",IntegerType(),True),\n",
    "                       StructField(\"mngr_id\",IntegerType(),True)])\n",
    "\n",
    "df = spark.createDataFrame(data,mySchema)\n",
    "df.show()\n",
    "\n",
    "# Create another Dataframe\n",
    "\n",
    "data1=[(19 ,'Sohan',50000, 18),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(20 ,'Sima',75000,  17)]\n",
    "\n",
    "df1 = spark.createDataFrame(data1,mySchema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bae377f-bbe9-45e8-8b76-a26c7312eeaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets Apply UNION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630429f9-2f72-4e0d-bcd8-acf43e093d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.union(df1).show()\n",
    "df.unionAll(df1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c08787bc-e9ee-47ed-9c58-abe7d28894ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets Apply UNION_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "489ec70e-af15-4878-9a70-b68ef7b66625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.unionAll(df1).show()\n",
    "df.unionAll(df1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef08f631-3eaf-484a-9eee-0c34c7acaac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "FROM ABOVE , we can see UNION and UNION_ALL are same PySpark - EVEN DUPLICATE RECORD also got unioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b463dc-215c-4c98-aebf-c8e8057ccd2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets Try UNION and UNION_ALL in Spark SQL  \n",
    "For that convert both df in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990ff86a-cd9e-43e1-bdf8-abef0dd201bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"table1\")\n",
    "df1.createOrReplaceTempView(\"table2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a26ffa15-1914-4a25-8910-e528cb4387bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM table1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0edd15c4-fd36-4809-a279-787af0dfefed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM table2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0183dcbc-980c-41dc-90aa-334e26c277df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "While Doing Union in sql Duplicate Record Gets Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1e94123-fd5d-4fde-ab77-f731abf9054f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# union()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          SELECT * FROM table1\n",
    "          UNION \n",
    "          SELECT * FROM table2\n",
    "          \"\"\").show()\n",
    "\n",
    "# No. Of Records \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          SELECT * FROM table1\n",
    "          UNION \n",
    "          SELECT * FROM table2\n",
    "          \"\"\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3fbc052-c945-4994-ba43-32efcddf000c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In UNION ALL, it Do not Removes Duplicate records Even In SQL Or SPARK SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1556f4dd-4172-4aa1-938e-f8da178b09d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# union all()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          SELECT * FROM table1\n",
    "          UNION ALL \n",
    "          SELECT * FROM table2\n",
    "          \"\"\").show()\n",
    "\n",
    "# No. Of Records \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          SELECT * FROM table1\n",
    "          UNION ALL\n",
    "          SELECT * FROM table2\n",
    "          \"\"\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0f4ecac-6ed1-470e-8c7d-074ac84cc478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What Will Happen it the order Of Column or Schema Of 2 df is different which will about to be union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf2b257-9ca3-42f2-afca-49fb9f14808d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wrong_column_data=[(19 ,50000, 18,'Sohan'),\n",
    "(20 ,75000,  17,'Sima')]\n",
    "\n",
    "wrong_schema = \"id int,sal int,mngr_id int,name string\"\n",
    "\n",
    "df3 = spark.createDataFrame(wrong_column_data,wrong_schema)\n",
    "df3.show()\n",
    "df3.printSchema()\n",
    "\n",
    "# df Data\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3df88621-6695-4284-a70a-348d9003a66b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.union(df3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a313c6db-59bf-4a9c-b5ce-cdd8387d282b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can resolve above issue using \"unionByName\"  \n",
    "\"UnionByName\" Only Works in case Of Change in Order of Columns but Columns Should be same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3782e02-9c52-44d0-b638-c8a05a8fbf91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.unionByName(df3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6025eda8-fb91-49b1-a28a-908f2a99c4cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If we try to union 2 dataframe with different schema, we will Get Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e6f98b0-9a54-412b-879b-d4a5bb8eb938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wrong_column_data2=[(19 ,50000, 18,'Sohan',99),\n",
    "(20 ,75000,  17,'Sima',99)]\n",
    "\n",
    "wrong_schema2 = \"id int,sal int,mngr_id int,name string, house_id int\"\n",
    "\n",
    "df4 = spark.createDataFrame(wrong_column_data2,wrong_schema2)\n",
    "df4.show()\n",
    "df4.printSchema()\n",
    "\n",
    "# df Data\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81dc5be3-a687-4dc1-818d-3e0fea4b04c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.unionByName(df4).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20dbf9a9-8412-47c4-9960-6ef3efb4c480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.union(df4).shwo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1e86761-6172-4ec1-9971-cc8784200a76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4727873041286068,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "UNION Vs UNION-ALL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
