{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e065144c-e8d0-48fa-898a-ab979d23056e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dd93064-024e-4179-a6a6-51a986585c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading Data 'flight_csv_data' from 'workspace.default.flight_csv_data' path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9549d537-b352-420a-badb-d7d5ce41d1e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .load(\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/flight_csv_data.csv\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd7bdf12-3825-48cd-b52b-e94fdaecb806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7bac03c-51a2-4c4e-9d79-4869d238e9ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**If We have our own Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e58b2d7b-8a40-46cd-93c3-3e2e2334fcf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mySchema = \"DEST_COUNTRY_NAME string, ORIGIN_COUNTRY_NAME string, count int\"\n",
    "\n",
    "df1 = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\",\"false\")\\\n",
    "  .schema(mySchema)\\\n",
    "  .option(\"mode\",\"PERMISSIVE\")\\\n",
    "  .load(\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/flight_csv_data.csv\")\n",
    "\n",
    "df1.show(5)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a85d5a8a-92bd-4880-8f95-dd023faf483d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**If We want to Skip Row while Reading Data (when 1st line is header but we want to give our own Schema)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa53802-bcad-48c8-971e-1f126e85024a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mySchema = \"DEST_COUNTRY_NAME string, ORIGIN_COUNTRY_NAME string, count int\"\n",
    "\n",
    "df2 = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\",\"false\")\\\n",
    "  .option(\"skipRows\",1)\\\n",
    "  .schema(mySchema)\\\n",
    "  .option(\"mode\",\"PERMISSIVE\")\\\n",
    "  .load(\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/flight_csv_data.csv\")\n",
    "\n",
    "df2.show(5)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a13054af-3682-4ca8-8def-09a3134fab35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " reading data chatgpt way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89237b8b-69b1-4ef0-9df2-7ef726df389b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(1,\"ABC\",20),\n",
    "        (2,\"CDE\",50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21a9e696-5a0d-4b73-9206-fa24cc8dcb2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Different way of Creating Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6dc1c38-4cb2-42b3-9d86-d5839406b380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mySchema1 = \"id int,name string, sal int\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "mySchema2 = StructType([StructField(\"id\",IntegerType(),True),\n",
    "                        StructField(\"name\",StringType(),True),\n",
    "                        StructField(\"sal\",IntegerType(),True)])\n",
    "\n",
    "mySchema3 = [\"id\",\"name\",\"sal\"]\n",
    "\n",
    "# Creating dataframe using Schema\n",
    "df1  = spark.createDataFrame(data,mySchema1)\n",
    "df2  = spark.createDataFrame(data,mySchema2)\n",
    "df3  = spark.createDataFrame(data,mySchema3)\n",
    "\n",
    "df1.show()\n",
    "df1.printSchema()\n",
    "df2.show()\n",
    "df2.printSchema()\n",
    "df3.show()\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f4690b6-f23a-45a9-824e-bf53cbf439a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creting a new Row and making it to  dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c6960f-e6c4-48d1-a034-baa988f906f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_row = Row(3,\"xyz\",88)\n",
    "\n",
    "df4 = spark.createDataFrame([new_row],mySchema1)\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f89c6e8c-d5b7-4759-9477-26e3437582ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "While Reading Data , Data is seprated with Some other **DELIMETER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ed8b64-f052-454a-a0b5-c03cf00001e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"delimeter\",\",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .load(\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/flight_csv_data.csv\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1e4254-9f2e-4167-88f9-fc82f4e731ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Different Read Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc2da170-0263-4d5c-b159-eb06f2a85699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PERMISSIVE Mode \n",
    "mySchema = \"DEST_COUNTRY_NAME string, ORIGIN_COUNTRY_NAME string, count int\"\n",
    "\n",
    "\n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "    .option(\"inferSchema\",\"false\")\\\n",
    "    .schema(mySchema)\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .load(\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/flight_csv_data.csv\")\n",
    "\n",
    "df.show(3)\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18b04537-5953-4119-820f-867549b09b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mySchema = \"DEST_COUNTRY_NAME string, ORIGIN_COUNTRY_NAME string, count int\"\n",
    "\n",
    "# FAILFAST Mode \n",
    "\n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "    .option(\"inferSchema\",\"false\")\\\n",
    "    .schema(mySchema)\\\n",
    "    .option(\"mode\",\"FAILFAST\")\\\n",
    "    .load(\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/flight_csv_data.csv\")\n",
    "\n",
    "df.show(3)\n",
    "df.printSchema()\n",
    "df.count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b26763-1378-455d-a5b4-266503ae1c21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dropmalformed Mode \n",
    "mySchema = \"DEST_COUNTRY_NAME string, ORIGIN_COUNTRY_NAME string, count int\"\n",
    "\n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "    .option(\"inferSchema\",\"false\")\\\n",
    "    .schema(mySchema)\\\n",
    "    .option(\"mode\",\"DROPMALFORMED\")\\\n",
    "    .load(\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/flight_csv_data.csv\")\n",
    "\n",
    "df.show(3)\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5339058d-b982-4450-a013-a3d0a5bdd8c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### LIMIT AND COLLECT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c21b31-3ebb-499b-9147-481144fd48ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "temp_df = df.limit(4)\n",
    "temp_df.show()\n",
    "\n",
    "collect_list = temp_df.collect()\n",
    "\n",
    "for i in collect_list:\n",
    "    print(i)\n",
    "    print(i.ORIGIN_COUNTRY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7781d21f-20fa-40eb-9c28-454669546898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Converting a Dataframe to a table of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318af9e2-6029-4096-b8e0-97134a765652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "489e3628-1317-4b5b-a72c-c7585f021bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from df_view limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b869fe2e-96f6-4c77-865c-cc21e2efe01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4852943585062992,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Reading Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
