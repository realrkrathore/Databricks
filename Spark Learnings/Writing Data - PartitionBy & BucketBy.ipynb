{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa684083-45ae-4c3a-95c9-3ab3f17a950b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b51990-0f81-48ab-b305-ece4d632e06d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Manish\",   26,  75000,  \"INDIA\",  \"m\"),\n",
    "    (2, \"Nikita\",   23, 100000,  \"USA\",    \"f\"),\n",
    "    (3, \"Pritam\",   22, 150000,  \"INDIA\",  \"m\"),\n",
    "    (4, \"Prantosh\", 17, 200000,  \"JAPAN\",  \"m\"),\n",
    "    (5, \"Vikash\",   31, 300000,  \"USA\",    \"m\"),\n",
    "    (6, \"Rahul\",    55, 300000,  \"INDIA\",  \"m\"),\n",
    "    (7, \"Raju\",     67, 540000,  \"USA\",    \"m\"),\n",
    "    (8, \"Praveen\",  28,  70000,  \"JAPAN\",  \"m\"),\n",
    "    (9, \"Dev\",      32, 150000,  \"JAPAN\",  \"m\"),\n",
    "    (10,\"Sherin\",   16,  25000,  \"RUSSIA\", \"f\"),\n",
    "    (11,\"Ragu\",     12,  35000,  \"INDIA\",  \"f\"),\n",
    "    (12,\"Sweta\",    43, 200000,  \"INDIA\",  \"f\"),\n",
    "    (13,\"Raushan\",  48, 650000,  \"USA\",    \"m\"),\n",
    "    (14,\"Mukesh\",   36,  95000,  \"RUSSIA\", \"m\"),\n",
    "    (15,\"Prakash\",  52, 750000,  \"INDIA\",  \"m\")\n",
    "]\n",
    "\n",
    "mySchema = \"emp_id int, name string, age int, salary int, country string, gender string\"\n",
    "\n",
    "df = spark.createDataFrame(data, mySchema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "361f4eeb-742a-4b6f-a948-6d8803aae2ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"path\",\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/outputs/\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f62b9376-7713-45b2-a3b8-0a9ac02be615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As Our Data Frame have multiple partitions so , it is save in multiple CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8616d5-6d3e-4b32-b144-d87e3d7fdaa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls /Volumes/workspace/default/rajatlearningdata/PySpark_data/outputs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45f9e49a-2641-47ae-8756-520f35fa0e68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets save Whole data/dataframe as only 2 partitions and save them as 2 CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "522e5fd7-129e-43aa-b1f6-65ad590051ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.repartition(2).write.format(\"csv\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .option(\"path\",\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/outputs/\")\\\n",
    "  .save()\n",
    "\n",
    "  # OR\n",
    "\n",
    "  # df.repartition(2).write.format(\"csv\") \\\n",
    "  #   .option(\"header\", \"true\") \\\n",
    "  #   .mode(\"overwrite\") \\\n",
    "  #   .save(\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/outputs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c50dda03-b54b-4a3c-8256-950e663b6cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now Only data/dataframe got saved into only 2 files (part-00000 & part-00000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0f24cc4-3192-4a2f-87da-2f1fbcdfab3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls /Volumes/workspace/default/rajatlearningdata/PySpark_data/outputs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bd33131-92be-4362-b7ad-37c20dc66ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Similiarly We can use other MODES like :\n",
    "1. append\n",
    "2. overwrite\n",
    "3. errorIfExists\n",
    "4. ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60147b98-c17f-4c1b-8400-f15016fc16fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PartitionBy and BucketBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ffafdc-e477-4292-a559-a477286f4046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e225840b-6c36-4e3c-a2c6-89e4b96b2b2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**partitionBy()** : Now Lets save Data in Seprate Folder According To Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74e94f40-10dd-4d47-aa48-1b6d560ec7d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"csv\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .partitionBy(\"country\")\\\n",
    "  .option(\"path\",\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/outputs/partitionBy/\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebde9e5-f8fb-4b24-a53d-2ae9a36e92dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls /Volumes/workspace/default/rajatlearningdata/PySpark_data/outputs/partitionBy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b575086-80d5-4b57-8ebf-06b7d255a0a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can See data got stored in seprate country folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "477b8ace-49c4-4329-9c3f-5ff138b84899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now Lets Try To save data only in 2 partitions for Each Country in Different Country Folder (repatition and partitionBy together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e585d2-39c3-40b3-9050-339ca99dc5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.repartition(2).write.format(\"csv\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .partitionBy(\"country\")\\\n",
    "  .option(\"path\",\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/outputs/partitionBy/\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f249047-42e1-474a-84aa-5edc42dafbf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets Partition Data According to COUNTRY AND THEN according to GENDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7330202d-bc9f-4194-b041-33ee4ef3cee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.repartition(2).write.format(\"csv\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .partitionBy(\"country\",\"gender\")\\\n",
    "  .option(\"path\",\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/outputs/partitionBy/\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340fc9bc-80c2-4142-9b86-8761cf5ee282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls dbfs:/Volumes/workspace/default/rajatlearningdata/PySpark_data/outputs/partitionBy/country=INDIA/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e56d03b-1a41-476f-99bf-d30e47ba627d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Above We can See inside each Country Folder we have 2 gender Folder (gender=f, gender=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44cc92b7-d9fc-477f-a1f8-a6f027665011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### bucketBy(no_of_buckets,\"col_name\") : Don't support CSV format, only parquet and ORC\n",
    "- .saveAsTable(), create MANAGED TABLE (managed by unity catalog), so we cannot provide \"\"\"option(\"path\",\"/filepaath\")\"\"\" becoz it is way of creating EXTERNAL TABLE\n",
    "- when we want to segrigate data on A COLUMN, but that column have HIGH CARDENALITY , we use bucketby()\n",
    "- let try to bucket the data/dataframe on EMP_ID column in 3 buckets\n",
    "- You generally CANNOT use bucketBy() in Databricks when Unity Catalog is enabled.\n",
    "But there is one narrow scenario where bucketBy() still works, Using Hive Metastore (legacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d443c9f8-d1ae-436d-8c8e-e9c4b6c45e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .option(\"path\",\"/Volumes/workspace/default/rajatlearningdata/PySpark_data/outputs/bucketBy/\")\\\n",
    "  .bucketBy(3,\"emp_id\")\\\n",
    "  .saveAsTable(\"butcketBy_id_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce32b77f-67e8-4b1c-84a3-4be6f1ef0946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Above Code can work If we use HIVE METASTORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053c2ca3-4e7d-4990-86a4-f801d7eb0317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .format(\"parquet\") \\\n",
    "  .bucketBy(3, \"emp_id\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"bucketby_id_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54575c2b-d855-4a74-93b9-c8b9d66cf43d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Above Error Explain :\n",
    "\n",
    "In Databricks Unity Catalog (UC):\n",
    "\n",
    "‚úÖ Managed tables ‚Üí ONLY Delta format\n",
    "\n",
    "‚ùå Parquet / CSV / ORC not allowed for managed tables\n",
    "\n",
    "saveAsTable() creates a managed table by default\n",
    "\n",
    "üëâ You are asking UC to create a managed Parquet table, which is not permitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8368b0fb-4491-4a0f-8386-2e5b45743775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .bucketBy(3, \"emp_id\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"bucketby_id_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf065194-dc0f-40c6-b7c0-0b54ca543238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7786549017862642,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Writing Data - PartitionBy & BucketBy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
