{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be9721e-cc1a-49b0-89e6-3944ad960b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b90be797-51dc-45df-a810-d243302543c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_data = [\n",
    "    (1,'manish', 50000,'IT','m'),\n",
    "    (2,'vikash', 60000,'sales','m'),\n",
    "    (3,'raushan',70000,'marketing','m'),\n",
    "    (4,'mukesh',80000,'IT','m'),\n",
    "    (5,'priti', 90000,'sales','f'),\n",
    "    (6,'nikita',45000,'marketing','f'),\n",
    "    (7,'ragini',55000,'marketing','f'),\n",
    "    (8,'rashi',100000,'IT','f'),\n",
    "    (9,'aditya',65000,'IT','m'),\n",
    "    (10,'rahul',50000,'marketing','m'),\n",
    "    (11,'rakhi',50000,'IT','f'),\n",
    "    (12,'akhilesh',90000,'sales','m')\n",
    "]\n",
    "\n",
    "mySchema = [\"id\",\"name\",\"salary\",\"department\",\"gender\"]\n",
    "\n",
    "\n",
    "emp_df = spark.createDataFrame(emp_data,mySchema)\n",
    "\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "953b4350-c92d-44f1-91d8-bffc105fb146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### rank() , dense_rank() , row_number()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afdf5c06-28bc-4e9f-b5b2-df90b064dd98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Applying Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d9fa47e-907e-48d2-ab86-e3da4bb674f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(desc(\"salary\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2453124b-3d5e-4481-b3ea-2c968445560b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "row_number() : Ranking the Salary in Each Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91557ffc-3a20-45e5-b8d3-c596371efca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = emp_df.withColumn('Row_number',row_number().over(window_spec))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92bb4f73-c3a3-4a86-b8b7-1a8ac85bae83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets Use Rank Function to Rank() and Dense_Rank() salary in Each Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dfaaa8f-fff3-4666-81a5-9341a4939a52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df2 = emp_df.withColumn(\"Rank\",rank().over(window_spec))\\\n",
    "  .withColumn(\"Dense_Rank\",dense_rank().over(window_spec))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "404f12b7-860b-4ddc-b7f9-dd6cdba36928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Partitioning Data On Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0de0ce5-6768-4ba5-81d0-7dbd4fdf5043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\",\"gender\").orderBy(desc(\"salary\"))\n",
    "\n",
    "# OR We can Use\n",
    "#window_spec = Window.partitionBy([\"department\",\"gender\"]).orderBy(desc(\"salary\"))\n",
    "\n",
    "df3 = emp_df.withColumn(\"Row_number\",row_number().over(window_spec))\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c4023e-ef98-48f4-b8c3-271d2a1cd61a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Ordering Data On Multiple-Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c76d98-fd37-4c7e-8d86-357ab9f25737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(desc(\"salary\"),desc(\"id\"))\n",
    "\n",
    "# OR We can Use\n",
    "#window_spec = Window.partitionBy(\"department\").orderBy([desc(\"salary\"),desc(\"id\")])\n",
    "\n",
    "df4 = emp_df.withColumn(\"Row_number\",row_number().over(window_spec))\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49d97263-4669-4e6b-9780-cc720004e6c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### LEAD - and - LAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e35f86-acab-46d4-86f7-3d5d00f46370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_data = [\n",
    "    (1,\"iphone\",\"01-01-2023\",1500000),\n",
    "    (2,\"samsung\",\"01-01-2023\",1100000),\n",
    "    (3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "    (1,\"iphone\",\"01-02-2023\",1300000),\n",
    "    (2,\"samsung\",\"01-02-2023\",1120000),\n",
    "    (3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "    (1,\"iphone\",\"01-03-2023\",1600000),\n",
    "    (2,\"samsung\",\"01-03-2023\",1080000),\n",
    "    (3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "    (1,\"iphone\",\"01-04-2023\",1700000),\n",
    "    (2,\"samsung\",\"01-04-2023\",1800000),\n",
    "    (3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "    (1,\"iphone\",\"01-05-2023\",1200000),\n",
    "    (2,\"samsung\",\"01-05-2023\",980000),\n",
    "    (3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "    (1,\"iphone\",\"01-06-2023\",1100000),\n",
    "    (2,\"samsung\",\"01-06-2023\",1100000),\n",
    "    (3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "cols = [\"product_id\",\"product_name\",\"sales_date\",\"sales\"]\n",
    "\n",
    "sales_df = spark.createDataFrame(product_data,cols)\n",
    "\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dccf9fc-f8b7-4bcd-9c77-687682d7e5a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "lag() : Q:> What is The Loss Or Gain As Compared to Previous Month sales for Each Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d4f6d0-e778-4c53-b9b5-5939871280d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col\n",
    "\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(\"sales_date\")\n",
    "\n",
    "lag_df = sales_df.withColumn(\"Loss_Gain\",col(\"sales\") - (lag(\"sales\",1).over(window_spec)))\n",
    "\n",
    "lag_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc3153f8-f336-4a1b-be95-7524f504646f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "lead() : find the Difference between current month and NEXT month sale for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2788719-1d6b-4bc0-9af1-b56686e014f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(desc(\"sales_date\"))\n",
    "\n",
    "df3 = sales_df.withColumn(\"Sales_diff\",col(\"sales\") - lead(\"sales\",1).over(window_spec))\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67fdd397-0fd8-408e-8717-49090e188696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Range : rowsBetween()\n",
    "- unboundedPreceding : all rows till current row (including Current Row)\n",
    "- unboundedFollowing : all rows after/next current row (including Current Row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f4e3dde-8eb2-4f5c-8aa7-72ee17be0ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Q : Find the Difference in sales, of Each Product from there first month sale to latest sales for each product (there will be only 1 lastest(last) sales date for Each product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e41afe-d159-4b6a-9f4c-25823dcefb31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(\"sales_date\")\n",
    "\n",
    "# Lets See FIRST month sales and LATEST (or LAST) month sales for Each Product\n",
    "\n",
    "df4 = sales_df.withColumn('first_month_sale',first('sales').over(window_spec))\\\n",
    "    .withColumn('latest_sales',last('sales').over(window_spec))\n",
    "df4.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84284889-16e6-4dcb-b734-323c4a794822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Above we can see first_month_sales is Correct but last/latest_month_sales is wrong as latest_sales should be also same for each product. We can Fix it using unboundedFollowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1630e3-ae77-445a-acc3-bc679b4e2aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(\"sales_date\").rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "\n",
    "# Lets See FIRST month sales and LATEST (or LAST) month sales for Each Product\n",
    "\n",
    "df4 = sales_df.withColumn('first_month_sale',first('sales').over(window_spec))\\\n",
    "    .withColumn('latest_sales',last('sales').over(window_spec))\n",
    "df4.show()\n",
    "\n",
    "# Now we can calculate Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea04354f-670d-41df-aba2-b8359bf1cdf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df5 = df4.withColumn('sales_diff',col(\"first_month_sale\") - col(\"latest_sales\")).drop(\"first_month_sale\",\"latest_sales\")\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "513763a4-5a40-43b5-93e7-c2f23b018721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46ac9c5e-4a14-4b65-b7c9-8d866a27b8cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Q> Send a mail to Employee who came to office but not completed 8 hourse in office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b1c622c-ca37-4f08-9ec6-9bcd197c8a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24cde751-3da8-4122-b63e-ea93846c99d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    # emp_id, emp_name, check_in, check_out, work_date\n",
    "    (101, \"Rajat\", \"09:00:00\", \"11:30:00\", \"2025-01-02\"),\n",
    "    (101, \"Rajat\", \"12:00:00\", \"15:00:00\", \"2025-01-02\"),\n",
    "    (101, \"Rajat\", \"15:30:00\", \"17:00:00\", \"2025-01-02\"),\n",
    "\n",
    "    (102, \"Amit\",  \"09:15:00\", \"13:00:00\", \"2025-01-02\"),\n",
    "    (102, \"Amit\",  \"14:00:00\", \"18:30:00\", \"2025-01-02\"),\n",
    "\n",
    "    (103, \"Neha\",  \"10:00:00\", \"13:00:00\", \"2025-01-02\"),\n",
    "    (103, \"Neha\",  \"14:00:00\", \"16:00:00\", \"2025-01-02\"),\n",
    "\n",
    "    (104, \"Pooja\", \"09:30:00\", \"12:30:00\", \"2025-01-02\"),\n",
    "    (104, \"Pooja\", \"13:00:00\", \"18:00:00\", \"2025-01-02\"),\n",
    "\n",
    "    (105, \"Arjun\", \"11:00:00\", \"14:00:00\", \"2025-01-02\"),\n",
    "    (105, \"Arjun\", \"15:00:00\", \"17:00:00\", \"2025-01-02\")\n",
    "]\n",
    "\n",
    "mySchema= [\"emp_id\", \"emp_name\", \"check_in_time\", \"check_out_time\", \"work_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data,mySchema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "758cbeaf-fc18-40ae-911c-860b7c14e8d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"emp_id\").orderBy(\"check_in_time\").rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "\n",
    "temp_df = df.withColumn(\"in_time\", first(\"check_in_time\").over(window_spec))\\\n",
    "    .withColumn(\"out_time\", last(\"check_out_time\").over(window_spec))\n",
    "\n",
    "temp_df2 = temp_df.withColumn(\"in_time\", to_timestamp(\"in_time\", \"HH:mm:ss\"))\\\n",
    "    .withColumn(\"out_time\", to_timestamp(\"out_time\", \"HH:mm:ss\"))\n",
    "\n",
    "temp_df2 = temp_df2.withColumn(\"work_hours\", (unix_timestamp(\"out_time\") - unix_timestamp(\"in_time\")) / 3600)\n",
    "\n",
    "temp_df2.show()\n",
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f52fd07d-fa9d-4965-8bbc-e0b50fc0806a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate Difference between time\n",
    "\n",
    "defaulter_df = temp_df2.filter(col('work_hours') < 8).drop(\"check_in_time\",\"check_out_time\",\"in_time\",\"out_time\").distinct()\n",
    "\n",
    "\n",
    "defaulter_df.show(truncate=False)\n",
    "defaulter_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ca987cc-3dfd-4cc6-9e78-0bca7183cf02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b40fa70a-936a-4a88-a3c8-9028ac95c71d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Q> Find The performance of sales based on last 3 month average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3bb17a3-baae-4070-9e60-77582620e27e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a661731e-3dc3-43ab-8884-5e75355b4e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_data = [\n",
    "    (1,\"iphone\",\"01-01-2023\",1500000),\n",
    "    (2,\"samsung\",\"01-01-2023\",1100000),\n",
    "    (3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "    (1,\"iphone\",\"01-02-2023\",1300000),\n",
    "    (2,\"samsung\",\"01-02-2023\",1120000),\n",
    "    (3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "    (1,\"iphone\",\"01-03-2023\",1600000),\n",
    "    (2,\"samsung\",\"01-03-2023\",1080000),\n",
    "    (3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "    (1,\"iphone\",\"01-04-2023\",1700000),\n",
    "    (2,\"samsung\",\"01-04-2023\",1800000),\n",
    "    (3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "    (1,\"iphone\",\"01-05-2023\",1200000),\n",
    "    (2,\"samsung\",\"01-05-2023\",980000),\n",
    "    (3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "    (1,\"iphone\",\"01-06-2023\",1100000),\n",
    "    (2,\"samsung\",\"01-06-2023\",1100000),\n",
    "    (3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "cols = [\"product_id\",\"product_name\",\"sales_date\",\"sales\"]\n",
    "\n",
    "sales_df = spark.createDataFrame(product_data,cols)\n",
    "\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "828ea408-5664-4a51-9c80-b9e9176a53fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Lets See Last 3 Month Average and then removing first 2 month record for each record (becoz here avg. is of (2 data) / 3 which is Wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e986ed-c19e-4ac8-9ed4-95a0ea9463d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"product_id\").orderBy(\"sales_date\").rowsBetween(-2,0)\n",
    "\n",
    "temp_df = sales_df.withColumn(\"3_month_sum\",sum(\"sales\").over(window_spec))\\\n",
    "    .withColumn(\"3_month_avg\",round(col(\"3_month_sum\")/3,2))\n",
    "\n",
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f1cf3a-86d8-4d39-9eff-f98f3eb83dcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now Remove first 2 record of Each Product - (Becoz Wrong AVG)\n",
    "\n",
    "window_spec2 = Window.partitionBy(\"product_id\").orderBy(\"sales_date\")\n",
    "\n",
    "row_number_df  = temp_df.withColumn(\"Row_Number\",row_number().over(window_spec2)).filter(col(\"Row_Number\") > 2).drop(\"Row_number\")\n",
    "row_number_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30f7225f-2efc-47b7-a4d4-66ef39c3c011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Window Functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
